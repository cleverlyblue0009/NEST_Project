"""
STEP 2: DATA QUALITY SIGNAL EXTRACTION (NO MAJOR CHANGES)
=========================================================

Status: UNCHANGED from previous version
(This module outputs percentage-based signals that ML anomaly detection uses)

No modifications needed for ML integration.
Signals are already in percentage format, suitable for anomaly detection.
"""

import os
import pandas as pd
import json
from pathlib import Path

def load_schema_map(schema_path="outputs/schema_map.json"):
    """Load the schema map generated by scan_schema.py"""
    if not os.path.exists(schema_path):
        print(f"❌ Schema map not found: {schema_path}")
        print("   Run scan_schema.py first.")
        return {}
    
    with open(schema_path, 'r') as f:
        return json.load(f)


def extract_study_site_from_path(file_path):
    """
    Robust multi-pass study/site extraction with fallback logic.
    
    Strategies (in order):
    1. Look for Study_XXX / Site_XXX patterns
    2. Infer from numeric patterns
    3. Use folder structure
    4. Extract from file name
    5. Create synthetic IDs (worst case)
    
    GUARANTEE: Never returns "Unknown"
    """
    parts = file_path.split(os.sep)
    study_id = None
    site_id = None
    
    # PASS 1: Explicit patterns
    for part in parts:
        if part.startswith("Study_") and "CPID" in part:
            study_id = part.split("_CPID")[0]
        elif part.startswith("Study_"):
            study_id = part
        
        if part.startswith("Site_"):
            site_id = part
        elif "site" in part.lower() and len(part) < 25:
            site_id = part
    
    # PASS 2: Numeric pattern
    if study_id is None:
        for part in parts:
            if part.isdigit() and 1 <= int(part) <= 999:
                study_id = f"Study_{part.zfill(3)}"
                break
    
    # PASS 3: Parent folder
    if site_id is None and len(parts) >= 2:
        potential_site = parts[-2]
        if potential_site != "." and not potential_site.startswith("Study"):
            site_id = potential_site
    
    # PASS 4: File name
    if site_id is None and len(parts) > 0:
        file_name = parts[-1]
        if "site" in file_name.lower():
            site_id = file_name.replace(".csv", "").replace(".xlsx", "").replace(".xls", "").replace(".xlsm", "")
    
    # PASS 5: Synthetic
    if study_id is None:
        hash_val = abs(hash(file_path)) % 10000
        study_id = f"Study_{hash_val:04d}"
        print(f"⚠ SYNTHETIC study_id: {file_path} → {study_id}")
    
    if site_id is None:
        hash_val = abs(hash(file_path)) % 1000
        site_id = f"Site_{hash_val:03d}"
        print(f"⚠ SYNTHETIC site_id: {file_path} → {site_id}")
    
    # Standardize format
    study_id = study_id.replace(" ", "_").strip()
    site_id = site_id.replace(" ", "_").strip()
    
    return study_id, site_id


def detect_signals(data_dir="data", schema_map=None):
    """
    Scan all files and extract clinical data quality signals (PERCENTAGES).
    
    Signals detected (as PERCENTAGES):
    1. Missing Pages: % of records with missing page status
    2. Missing Visits: % of records with missing visit data
    3. Unresolved EDRR: % of records with open queries
    4. Uncoded Terms: % of records with uncoded medical terms
    5. Pending SAE Review: % of records with pending SAE status
    
    Args:
        data_dir (str): Root data directory
        schema_map (dict): File schema from scan_schema.py
    
    Returns:
        pd.DataFrame: Signal-level data with PERCENTAGES
    """
    
    if schema_map is None:
        schema_map = {}
    
    signals_list = []
    data_path = Path(data_dir)
    
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if not (file.lower().endswith(('.csv', '.xlsx', '.xls', '.xlsm'))):
                continue
            
            full_path = Path(root) / file
            rel_path = str(full_path.relative_to(data_path))
            
            # Extract study & site using robust multi-pass logic
            study_id, site_id = extract_study_site_from_path(rel_path)
            
            try:
                # Read file
                if file.lower().endswith('.csv'):
                    df = pd.read_csv(full_path, encoding='utf-8')
                else:
                    xls = pd.ExcelFile(full_path)
                    first_sheet = xls.sheet_names[0] if xls.sheet_names else None
                    if not first_sheet:
                        continue
                    df = pd.read_excel(full_path, sheet_name=first_sheet)
                
                if df.empty:
                    continue
                
                # Normalize column names
                df.columns = df.columns.str.lower().str.strip()
                row_count = len(df)
                
                # SIGNAL 1: Missing Pages
                crf_status_cols = [col for col in df.columns if 'page' in col or 'form' in col or 'status' in col]
                missing_pages_count = 0
                
                if crf_status_cols:
                    for col in crf_status_cols:
                        missing_pages_count += df[col].isna().sum()
                        missing_pages_count += (df[col].astype(str).str.lower().isin(['missing', 'incomplete', 'pending'])).sum()
                
                missing_pages_pct = (missing_pages_count / row_count * 100) if row_count > 0 else 0
                
                # SIGNAL 2: Missing Visits
                visit_cols = [col for col in df.columns if 'visit' in col]
                missing_visits_count = 0
                
                if visit_cols:
                    for col in visit_cols:
                        missing_visits_count += df[col].isna().sum()
                
                missing_visits_pct = (missing_visits_count / row_count * 100) if row_count > 0 else 0
                
                # SIGNAL 3: Unresolved EDRR
                edrr_cols = [col for col in df.columns if 'edrr' in col or 'query' in col or 'issue' in col or 'discrepancy' in col]
                unresolved_edrr_count = 0
                
                if edrr_cols:
                    for col in edrr_cols:
                        unresolved_edrr_count += df[col].isna().sum()
                        unresolved_edrr_count += (df[col].astype(str).str.lower().isin(['open', 'pending', 'unresolved'])).sum()
                
                unresolved_edrr_pct = (unresolved_edrr_count / row_count * 100) if row_count > 0 else 0
                
                # SIGNAL 4: Uncoded Terms
                code_cols = [col for col in df.columns if 'code' in col or 'meddra' in col or 'whodd' in col]
                uncoded_terms_count = 0
                
                if code_cols:
                    for col in code_cols:
                        uncoded_terms_count += df[col].isna().sum()
                        uncoded_terms_count += (df[col].astype(str).str.lower().isin(['', 'missing', 'not coded', 'pending'])).sum()
                
                uncoded_terms_pct = (uncoded_terms_count / row_count * 100) if row_count > 0 else 0
                
                # SIGNAL 5: Pending SAE Reviews
                sae_cols = [col for col in df.columns if 'sae' in col or 'serious' in col or 'adverse' in col]
                pending_sae_count = 0
                
                if sae_cols:
                    for col in sae_cols:
                        pending_sae_count += (df[col].astype(str).str.lower().isin(['pending', 'unreviewed', 'awaiting review'])).sum()
                
                pending_sae_pct = (pending_sae_count / row_count * 100) if row_count > 0 else 0
                
                # Build signal record with PERCENTAGES
                signal_record = {
                    'study_id': study_id,
                    'site_id': site_id,
                    'file_path': rel_path,
                    'file_name': file,
                    'row_count': row_count,
                    'missing_pages_pct': round(missing_pages_pct, 2),
                    'missing_visits_pct': round(missing_visits_pct, 2),
                    'unresolved_edrr_pct': round(unresolved_edrr_pct, 2),
                    'uncoded_terms_pct': round(uncoded_terms_pct, 2),
                    'pending_sae_pct': round(pending_sae_pct, 2),
                }
                signals_list.append(signal_record)
                
                print(f"✓ {rel_path}: Signals extracted")
            
            except Exception as e:
                print(f"⚠ {rel_path}: Could not extract signals ({type(e).__name__})")
                continue
    
    return pd.DataFrame(signals_list) if signals_list else pd.DataFrame()


def aggregate_by_study_site(signals_df):
    """
    Aggregate file-level signals to study and site level.
    Use MEAN for percentages (not sum).
    """
    
    if signals_df.empty:
        return pd.DataFrame(), pd.DataFrame()
    
    # Study-level aggregation
    study_signals = signals_df.groupby('study_id').agg({
        'missing_pages_pct': 'mean',
        'missing_visits_pct': 'mean',
        'unresolved_edrr_pct': 'mean',
        'uncoded_terms_pct': 'mean',
        'pending_sae_pct': 'mean',
        'file_path': 'count',
        'row_count': 'sum',
    }).rename(columns={'file_path': 'file_count'}).reset_index()
    
    # Site-level aggregation
    site_signals = signals_df.groupby(['study_id', 'site_id']).agg({
        'missing_pages_pct': 'mean',
        'missing_visits_pct': 'mean',
        'unresolved_edrr_pct': 'mean',
        'uncoded_terms_pct': 'mean',
        'pending_sae_pct': 'mean',
        'file_path': 'count',
        'row_count': 'sum',
    }).rename(columns={'file_path': 'file_count'}).reset_index()
    
    return study_signals, site_signals


def save_signals(signals_df, study_signals, site_signals, output_path="outputs/signals.csv"):
    """Save signal data to CSV files."""
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    signals_df.to_csv(output_path, index=False)
    print(f"✓ File-level signals saved to: {output_path}")
    
    study_path = output_path.replace('signals.csv', 'signals_study_level.csv')
    study_signals.to_csv(study_path, index=False)
    print(f"✓ Study-level signals saved to: {study_path}")
    
    site_path = output_path.replace('signals.csv', 'signals_site_level.csv')
    site_signals.to_csv(site_path, index=False)
    print(f"✓ Site-level signals saved to: {site_path}")


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("STEP 2: DATA QUALITY SIGNAL EXTRACTION")
    print("=" * 80)
    
    schema_map = load_schema_map()
    
    print("\nScanning for signals...")
    signals_df = detect_signals(data_dir="data", schema_map=schema_map)
    
    if signals_df.empty:
        print("\n⚠ No signals extracted. Check data files.")
    else:
        print(f"\n✓ Extracted signals from {len(signals_df)} files")
        
        study_signals, site_signals = aggregate_by_study_site(signals_df)
        
        print(f"✓ Aggregated to {len(study_signals)} studies and {len(site_signals)} sites")
        
        save_signals(signals_df, study_signals, site_signals)
        print("\n✓ All signals saved.\n")